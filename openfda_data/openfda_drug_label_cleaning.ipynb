{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e07f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029b398",
   "metadata": {},
   "source": [
    "# Drugs dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc88f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b40809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the drugs.csv from AWS\n",
    "drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6177189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: There are some issues with unfurling the openfda column, specifically\n",
    "\n",
    "# Unfurl the 'openfda' column\n",
    "drugs['openfda'] = drugs['openfda'].apply(lambda x: json.loads(x) if pd.notnull(x) and isinstance(x, str) else {})\n",
    "\n",
    "# Unfurl the nested JSON in the 'openfda' column and hold the index\n",
    "df_openfda = pd.json_normalize(drugs['openfda'])\n",
    "df_openfda.columns = [f\"openfda_{col}\" for col in df_openfda.columns]  # Rename to avoid collisions\n",
    "\n",
    "# Concatenate the expanded openfda data back with the original dataframe, retaining the index\n",
    "df_with_fully_expanded = pd.concat([drugs.drop(columns=['openfda']), df_openfda], axis=1)\n",
    "df_with_fully_expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6067180a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is somewhat custom for now. A lot of these columns have nested json. \n",
    "# Some we care about like \"active_ingredients\", others we don't really need to worry about.\n",
    "\n",
    "def explode_json_columns(drugs):\n",
    "    \n",
    "    # Start with products\n",
    "    drugs['products'] = drugs['products'].apply(lambda x: json.loads(x) if isinstance(x, str) else [])\n",
    "    df_exploded = drugs.explode('products').reset_index(drop=True)\n",
    "    df_products_expanded = pd.json_normalize(df_exploded['products'])\n",
    "    df_products = pd.concat([df_exploded.drop(columns=['products']), df_products_expanded], axis=1)\n",
    "    \n",
    "    # Now active_ingredients list\n",
    "    df_products['active'] = df_products['active_ingredients'].apply(lambda x: json.loads(x) if isinstance(x, str) else [])\n",
    "    df_exploded = df_products.explode('active_ingredients').reset_index(drop=True)\n",
    "    df_products_expanded = pd.json_normalize(df_exploded['active_ingredients'])\n",
    "    df_active = pd.concat([df_exploded.drop(columns=['active_ingredients', 'active']), df_products_expanded], axis=1)\n",
    "    df_active = df_active.rename(columns={'name': 'active_ingredients', 'strength': 'strength_active_ingredients'})\n",
    "    \n",
    "    # finally, explode submissions column\n",
    "    df_active['submissions'] = df_active['submissions'].apply(lambda x: json.loads(x) if isinstance(x, str) else [])\n",
    "    df_exploded = df_active.explode('submissions').reset_index(drop=True)\n",
    "    df_submissions_expanded = pd.json_normalize(df_exploded['submissions'])\n",
    "    df_submissions = pd.concat([df_exploded.drop(columns=['submissions']), df_submissions_expanded], axis=1)\n",
    "\n",
    "    return df_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66addd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_exploded = explode_json_columns(df_with_fully_expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix dates in submission_status_date\n",
    "\n",
    "drugs_exploded['submission_status_date'] = pd.to_datetime(drugs_exploded['submission_status_date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5fa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "drugs_exploded['application_number'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebfc865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see here we don't really care about the \"application_docs\" column\n",
    "# The reason this has duplicates is because it has different submission dates/numbers. \n",
    "# I plan to do some research on submission_type == 'ORIG' and submission_status_date so I can ensure \n",
    "# I get the original approval date. Also, note you may not be able to just throw away the other submissions as\n",
    "# they may signify updated materials in other columns. For drug formulation v1, this is less of a concern.\n",
    "\n",
    "\n",
    "drugs_exploded[drugs_exploded['application_number'] == 'ANDA076204']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bb9678",
   "metadata": {},
   "source": [
    "# Product Labels Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_labels.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548f8ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First drop all columns that end in _table because they're just the HTML behind the table in the label insert\n",
    "product_labels = product_labels.loc[:, ~product_labels.columns.str.endswith('_table')]\n",
    "product_labels.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ec309",
   "metadata": {},
   "source": [
    "## indications_and_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4164ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean indications and usage column\n",
    "\n",
    "product_label_test = product_labels[['id', 'openfda', 'indications_and_usage']]\n",
    "\n",
    "product_label_test['indications_and_usage'] = product_label_test['indications_and_usage'].astype(str)\n",
    "\n",
    "# First change all text to lowercase\n",
    "def text_lower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "product_label_test['indications_and_usage_lower'] = product_label_test['indications_and_usage'].apply(lambda x: text_lower(x))\n",
    "\n",
    "# remove punctuation\n",
    "punctuation = '!\"#$&\\'()*+-:;<=>?@[\\\\]^_`{|}~1234567890.'\n",
    "def remove_punct(text):\n",
    "    nopunct = \"\".join([char for char in text if char not in punctuation])\n",
    "    return nopunct\n",
    "\n",
    "product_label_test['indications_and_usage_lower'] = product_label_test['indications_and_usage_lower'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "\n",
    "# now tokenize the text\n",
    "def token(text):\n",
    "    t = WordPunctTokenizer().tokenize(text)\n",
    "    return t\n",
    "\n",
    "product_label_test['indications_and_usage_token'] = product_label_test['indications_and_usage_lower'].apply(lambda x: token(x))\n",
    "\n",
    "\n",
    "# remove stopwords\n",
    "#nltk.download('stopwords')\n",
    "#stopword = stopwords.words('english')\n",
    "#more_stopwords = [\"indications\", \"usage\", \"1\", \"use\", \"used\", \"directed\", \"nan\"]\n",
    "#stopword.extend(more_stopwords)\n",
    "\n",
    "\n",
    "# Keep adding to stopwords to see if we can get this to a good place\n",
    "stopword = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', \n",
    "'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', \n",
    "'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', \n",
    "'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    " 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are',\n",
    " 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
    " 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
    "'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', \n",
    "'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', \n",
    "'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', \n",
    "'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \n",
    "'s', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y',\n",
    " 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", \n",
    "'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn',\n",
    " \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", \n",
    "'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", \"indications\", \"usage\", \"1\", \n",
    "            \"use\", \"used\", \"directed\", \"nan\", \"direction\", \"help\", \"reduce\", \"prevent\", \"cause\", \"potentially\", \"us\", \n",
    "           \"may\", \"see\", \"clinical\", \"indicated\", \"treatment\", \"treat\", \"treated\", \"study\", \"tablet\", \n",
    "           \"years\", \"age\", \"patients\", \"year\", \"patient\", \"usp\", \"temporarily\", \"relief\", \"caused\", \"older\", \n",
    "           \"information\", \"temporary\", \"hour\", \"approved\", \"uses\", \"relieved\", \"relieve\", \"symptoms\", \"symptom\", \"associated\", \n",
    "           \"controlled\", \"control\", \"trial\", \"trials\", \"trialed\", \"helps\", \"helped\", \"help\", \n",
    "           \"extended\", \"release\", \"tablet\", \"available\", \"measure\", \"measures\", \"measured\", \"direction\", \"directions\", \n",
    "           \"adult\", \"soap\", \"water\", \"physician\", \"physicians\", \"factor\", \"factors\", \"dosage administration\", \"absolute\", \n",
    "           \"structure\", \"structures\", \"structured\", \"adjunctive therapy\", \"adjuvant therapy\", \"data\", \"local\", \"would\", \"expect\", \"expects\", \n",
    "           \"expected\", \"wide\", \"variety\", \"varieties\", \"also\", \"seen\", \"saw\", \"make\", \"made\"]\n",
    "\n",
    "def remove_stopwords(tokenized_text):\n",
    "    text = [word for word in tokenized_text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "product_label_test['indications_and_usage_nostopwords'] = product_label_test['indications_and_usage_token'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "\n",
    "# need to lemmatize \n",
    "nltk.download('wordnet')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(tokenized_text):\n",
    "    text = [lemmatizer.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "product_label_test['indications_and_usage_lemma'] = product_label_test['indications_and_usage_nostopwords'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "\n",
    "product_label_test['indications_and_usage_clean'] = product_label_test['indications_and_usage_lemma'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))\n",
    "\n",
    "product_label_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if wordcloud helps us refine stopwords a bit\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "result = product_label_test['indications_and_usage_clean'].astype('str')\n",
    "\n",
    "result = Counter(result)\n",
    "\n",
    "text = product_label_test['indications_and_usage_clean'].astype('str')\n",
    "\n",
    "print (\"There are\", text.nunique(), \"unique indications in this dataset given\", \n",
    "       product_label_test['id'].nunique(), \"drugs.\")\n",
    "\n",
    "stopwords = ['none']\n",
    "\n",
    "wordcloud = WordCloud(stopwords = stopwords, background_color = \"white\", max_font_size = 300, random_state = 42, \n",
    "                     width = 800, height = 500).generate_from_frequencies(result)\n",
    "\n",
    "plt.figure(figsize=[25,20])\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa406df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get some indication counts so we can take a look at how clean our data is\n",
    "\n",
    "text = product_label_test['indications_and_usage_clean'].astype('str') + ' '\n",
    "text = ' '.join(review for review in product_label_test.indications_and_usage_clean)\n",
    "\n",
    "\n",
    "counts = WordCloud(stopwords = stopwords).process_text(text)\n",
    "indication_counts = pd.Series(counts).to_frame().sort_values([0], ascending = False)\n",
    "indication_counts.reset_index(level=0, inplace = True)\n",
    "indication_counts = indication_counts.rename(columns={0:'count', 'index':'indication'})\n",
    "indication_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd05ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "indication_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff712e9d",
   "metadata": {},
   "source": [
    "# ChatGPT approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6177218",
   "metadata": {},
   "outputs": [],
   "source": [
    "indications = pd.read_csv('/Users/helena/Downloads/5000_drug_indications.csv')\n",
    "indications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d540f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_label_test_gpt = product_labels[['id', 'openfda', 'indications_and_usage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c89bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First change all text to lowercase\n",
    "def text_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "punctuation = '!\"#$&\\'()*+-:;<=>?@[\\\\]^_`{|}~1234567890.'\n",
    "def remove_punct(text):\n",
    "    return \"\".join([char for char in text if char not in punctuation])\n",
    "\n",
    "# Preprocess indications data\n",
    "indications['Drug Indications'] = indications['Drug Indications'].astype(str)\n",
    "indications['Drug Indications'] = indications['Drug Indications'].apply(lambda x: text_lower(x))\n",
    "indications['Drug Indications'] = indications['Drug Indications'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "# Create a set of indications for faster matching\n",
    "indications_set = set(indications['Drug Indications'])\n",
    "\n",
    "# Preprocess the product label text\n",
    "product_label_test_gpt['indications_and_usage'] = product_label_test_gpt['indications_and_usage'].astype(str)\n",
    "product_label_test_gpt['indications_and_usage'] = product_label_test_gpt['indications_and_usage'].apply(lambda x: text_lower(x))\n",
    "product_label_test_gpt['indications_and_usage'] = product_label_test_gpt['indications_and_usage'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "# Function to match drug indications in the original text\n",
    "def match_indication(text):\n",
    "    # Check for partial matches in the text\n",
    "    matches = [indication for indication in indications_set if indication in text]\n",
    "    # Return the matched indications as a comma-separated string\n",
    "    return ', '.join(matches) if matches else 'No match'\n",
    "\n",
    "# Apply the matching function to the original data\n",
    "product_label_test_gpt['indication'] = product_label_test_gpt['indications_and_usage'].apply(lambda x: match_indication(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec8fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not good...\n",
    "\n",
    "product_label_test_gpt.head(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
